{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8e93b8-f60a-4222-a156-51672f2ccdd4",
   "metadata": {},
   "source": [
    "# Notebook for TMC SaaS migration TOI demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487c833-cf74-475f-bf50-93bfd218c548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uname -a\n",
    "echo \"PWD=$PWD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0ad34-b2ed-4a9e-b4b9-5d9d2bd5d30a",
   "metadata": {},
   "source": [
    "## Clean up before execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09489acf-257b-4080-be46-a79d8f13be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 39568 Jun 26 06:31 \u001b[0m\u001b[01;31mdata_1750919468.tar.gz\u001b[0m\n",
      "ls: cannot access './data': No such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "nameprefix=$(date +%s)\n",
    "\n",
    "tar -zcf data_$nameprefix.tar.gz ./data\n",
    "ls -l data_$nameprefix.tar.gz\n",
    "\n",
    "rm -rf ./data\n",
    "ls -l ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694c149-9239-4220-b1eb-e83412f6bc51",
   "metadata": {},
   "source": [
    "## Connect to SaaS stack\n",
    "\n",
    "Export the necessary environment variables to initialize the CLI context and exchange the API access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4960105-4b30-4490-b286-fa625bc8cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV vars exported!\n"
     ]
    }
   ],
   "source": [
    "export TANZU_API_TOKEN=g69o-MNlS6aUrMPK0PZlAACbcEApcLQHzn9UXl86TuyAaV9vRRG-M8lNnN_ptBSW\n",
    "export ORG_NAME=trh\n",
    "export TANZU_CLI_CEIP_OPT_IN_PROMPT_ANSWER=no\n",
    "export TOKEN_URL=https://console-stg.tanzu.broadcom.com/csp/gateway/am/api/auth/api-tokens/authorize\n",
    "export REFRESH_TOKEN=$TANZU_API_TOKEN\n",
    "\n",
    "# Export filters\n",
    "export CLUSTER_NAME_FILTER=\"wc-02\"\n",
    "export TMC_MC_FILTER=\"sc-803\"\n",
    "\n",
    "\n",
    "echo \"ENV vars exported!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d25ba-9ec0-4b60-a6f2-dccec50025fc",
   "metadata": {},
   "source": [
    "Override the variables in STG environment, it's not required for customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5fec6398-8a64-4b0e-80c0-7f728751fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export CSP_URL=https://console-stg.tanzu.broadcom.com/csp/gateway/am/api/auth/api-tokens/authorize\n",
    "export TMC_ENDPOINT=${ORG_NAME}.tmc-dev.tanzu.broadcom.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb2203-1aaa-42c2-a6bc-cadbb35a4a94",
   "metadata": {},
   "source": [
    "Run **./001-base-saas_stack-connect.sh** to initialize the CLI context and exchange the API access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3169e98-0889-4cec-8b8c-73f498b32dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tanzu CLI context and exchange an API access token for directly calling the API endpoint with curl.\n",
      "Environment variables needed:\n",
      "TANZU_API_TOKEN=g69o-MNlS6aUrMPK0PZlAACbcEApcLQHzn9UXl86TuyAaV9vRRG-M8lNnN_ptBSW\n",
      "ORG_NAME=trh\n",
      "TANZU_CLI_CEIP_OPT_IN_PROMPT_ANSWER=no\n",
      "TOKEN_URL=https://console-stg.tanzu.broadcom.com/csp/gateway/am/api/auth/api-tokens/authorize\n",
      "REFRESH_TOKEN=g69o-MNlS6aUrMPK0PZlAACbcEApcLQHzn9UXl86TuyAaV9vRRG-M8lNnN_ptBSW\n",
      "[ok] Marking agreement as accepted.\n",
      "[ok] Successfully deleted context \"migration\"\n",
      "[ok] Marking agreement as accepted.\n",
      "[i] API token env var is set\n",
      "\n",
      "[ok] successfully created a TMC context\n",
      "[i] Fetching recommended plugins for active context 'migration'...\n",
      "[ok] All recommended plugins are already installed and up-to-date.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5915    0  5837  100    78  17269    230 --:--:-- --:--:-- --:--:-- 17500\n",
      "\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InNpZ25pbmdfNCJ9.eyJzdWIiOiJ2bXdhcmUuY29tOjFhOGQ0Y2YyLWZiZGYtNDk1ZC1hZTA2LTVjZmM1MjFhY2VlYyIsImlzcyI6Imh0dHBzOi8vY3NwLWdhei1jb3JlLmNvbnNvbGUtc3RnLnRhbnp1LmJyb2FkY29tLmNvbSIsImNvbnRleHRfbmFtZSI6ImI1ZmZmMDllLTIyMWQtNGEyMi05NGQ1LTIxNTMzYjc1ODFhOSIsImF6cCI6ImNzcF9zdGdfZ2F6X2ludGVybmFsX2NsaWVudF9pZCIsImF1dGhvcml6YXRpb25fZGV0YWlscyI6W10sImRvbWFpbiI6InZtd2FyZS5jb20iLCJjb250ZXh0IjoiNDY1MWZmYTgtM2I1Ny00YTU3LTk4ZGQtYTdmYzkzM2Q0MjU5IiwicGVybXMiOlsiZXh0ZXJuYWwvNWI5MTliZDktYjAyOS00NWM3LTgyOWQtMWEzMGZhZDI4MDhlL2Vuc2VtYmxlOmFkbWluIiwiZXh0ZXJuYWwvNjY4N2ZjYzYtMzQxNy00MDA2LWJmMzEtY2U4YzJhYjUzNmZjL2JlY3M6dXNlciIsImV4dGVybmFsL2Y1MmQzOWIwLWMyOTgtNGFkZi05YzZmLTBhNGEwNzM1MWNkNy9zZXJ2aWNlOmFkbWluIiwiY3NwOm9yZ19tZW1iZXIiLCJleHRlcm5hbC8yNTgzNDE5NS0xOWFhLTRmZmQtODkzMy1mNWYyMDA5NGFiMjQvc2VydmljZTphZG1pbiIsImNzcDpvcmdfb3duZXIiLCJleHRlcm5hbC8yNTgzNDE5NS0xOWFhLTRmZmQtODkzMy1mNWYyMDA5NGFiMjQvc2VydmljZTptZW1iZXIiLCJleHRlcm5hbC81YjkxOWJkOS1iMDI5LTQ1YzctODI5ZC0xYTMwZmFkMjgwOGUvZW5zZW1ibGU6dGFuenUtcGxhdGZvcm0tbWFuYWdlZCIsImV4dGVybmFsLzM5NzIxZDMyLTM5NjItNGE3NS04M2Q5LTliM2RhZTIzYzM5ZC90YXA6dmlld2VyIiwiZXh0ZXJuYWwvMzk3MjFkMzItMzk2Mi00YTc1LTgzZDktOWIzZGFlMjNjMzlkL3RhcDphZG1pbiIsImV4dGVybmFsL2Y1MmQzOWIwLWMyOTgtNGFkZi05YzZmLTBhNGEwNzM1MWNkNy9zZXJ2aWNlOm1lbWJlciIsImNzcDpvcmdfYWRtaW4iLCJleHRlcm5hbC8zOTcyMWQzMi0zOTYyLTRhNzUtODNkOS05YjNkYWUyM2MzOWQvdGFwOm1lbWJlciJdLCJleHAiOjE3NTA5MjE0NDMsImlhdCI6MTc1MDkxOTY0NCwianRpIjoiNTM5OTIzM2ItMTcyOS00Mjg4LWJlZDgtNThmMDA3ZWQ1MjdmIiwiYWNjdCI6InN6b3VAdm13YXJlLmNvbSIsInVzZXJuYW1lIjoic3pvdSJ9.DVS5LHTTV3Iu0dc1IgT3gF4gyANU4ViqJYGgsC-gKG7jugbJQSUg_qMwzpfBW54HgL03uRY1LKl2T19vdDCwI8BTjKl6Qw8Ll3QB8C2k-NqOefdzhaYmJkA4nyHQewzZL0XJvIJMse93qnI0hHm8UtVkWph5Z2OsUuNjjS2IBlSiyBeja_7ozPiI4vZ-GiH6NB3JONpBjKkJDdXW67kJuehpoQIVGqFOn6vtHcablOSD-OLlK9FSbt_q2E5vXOETEYUNpX5j6UlsUu7UHUyBRwHgwymt_UejZRHF4Zok0xGx-Oktaqzb5TnGq5y8y6yCUIrmmVxOlzm-DiVIbhN9_w\"\n"
     ]
    }
   ],
   "source": [
    "echo \"Create Tanzu CLI context and exchange an API access token for directly calling the API endpoint with curl.\"\n",
    "\n",
    "echo \"Environment variables needed:\"\n",
    "\n",
    "echo \"TANZU_API_TOKEN=$TANZU_API_TOKEN\"\n",
    "echo \"ORG_NAME=$ORG_NAME\"\n",
    "echo \"TANZU_CLI_CEIP_OPT_IN_PROMPT_ANSWER=$TANZU_CLI_CEIP_OPT_IN_PROMPT_ANSWER\"\n",
    "echo \"TOKEN_URL=$TOKEN_URL\"\n",
    "echo \"REFRESH_TOKEN=$REFRESH_TOKEN\"\n",
    "\n",
    "tanzu config eula accept\n",
    "./001-base-saas_stack-connect.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b9868-be07-4151-905c-aa24df135cc9",
   "metadata": {},
   "source": [
    "## Export the base resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a423b0e-5500-4005-b559-a6526955b888",
   "metadata": {},
   "source": [
    "### Export cluster group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f5fad7-f866-4ec1-a294-33d52a435179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting ClusterGroups from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported ClusterGroups from TMC SaaS: data/clustergroup/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Cluster Groups\n",
    "./002-base-clustergroups-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b161ab8-7340-4827-8862-2f0838da8b68",
   "metadata": {},
   "source": [
    "### Export workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efc042bc-03d0-4b4b-b5fd-43444b668963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Workspaces from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Workspaces from TMC SaaS: data/workspace/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Workspaces\n",
    "./003-base-workspaces-export.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a01d74-e735-4094-96d1-5b429c185ed7",
   "metadata": {},
   "source": [
    "## Export administration resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9fdfb3-7f4f-4fd6-b1d3-b6d1d5d1723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Customized Roles from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Customized Roles from TMC SaaS: data/role/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Roles\n",
    "./004-admin-roles-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd403d52-94fd-4d1c-be3e-ee2f24613e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Credentials from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Credentials from TMC SaaS: data/credential/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Credentials\n",
    "./005-admin-credentials-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0bee726-0d7a-4db2-8295-828c3bd93574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Admin Access from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Admin Access from TMC SaaS: data/credential-access/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Access\n",
    "./006-admin-access-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595cbadd-d5af-465d-85bc-3b7d1d34c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Proxy from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Proxy from TMC SaaS: data/proxy/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Proxy\n",
    "./007-admin-proxy-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31829768-4cde-40c2-a8c9-532ac9c65094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Image Registry from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Image Registry from TMC SaaS: data/image-registry/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Image Registry\n",
    "./008-admin-image-registry-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea179b2b-8c48-4c11-bf08-d766b54e3389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Admin Settings from TMC SaaS ...\n",
      "************************************************************************\n",
      "Exported Admin Settings from TMC SaaS: data/setting/*.yaml\n"
     ]
    }
   ],
   "source": [
    "# Export Settings\n",
    "./009-admin-settings-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27e1ec-ef92-4667-a677-2aa22d99cc99",
   "metadata": {},
   "source": [
    "## Export add-on resources of cluster group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9e9c631-2572-4e3f-b1ec-5bd83fdf0580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:37:37 INFO  [010] Export the cluster group secrets ...\n",
      "2025-06-26 06:37:41 INFO  [010] Export the cluster group secrets completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export k8s secret resources of cluster groups\n",
    "./010-clustergroup-secrets-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2eb65fc-e52e-4561-aa31-2b6a62721228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:37:50 INFO  [011] Export the cluster group secret exports ...\n",
      "2025-06-26 06:37:55 INFO  [011] Export the cluster group secret exports completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export k8s secret export resources of cluster groups\n",
    "./011-clustergroup-secret-exports-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "128387ad-15e7-40bb-af69-7149371c2b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:38:05 INFO  [012] Export the cluster group continuous deliveries ...\n",
      "2025-06-26 06:38:10 INFO  Export continuous delivery for cluster group 'default'\n",
      "2025-06-26 06:38:12 INFO  Export continuous delivery for cluster group 'test-cg'\n",
      "2025-06-26 06:38:14 INFO  Export continuous delivery for cluster group 'test-cg1'\n",
      "2025-06-26 06:38:16 INFO  Export continuous delivery for cluster group 'test-cg2'\n",
      "2025-06-26 06:38:19 INFO  [012] Export the cluster group continuous deliveries completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export fluxcd resources of cluster groups\n",
    "./012-clustergroup-continuous-deliveries-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "615be711-d69e-4c6d-8eb4-7d3072acf482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:38:24 INFO  [013] Export the cluster group repository credentials ...\n",
      "2025-06-26 06:38:28 INFO  [013] Export the cluster group repository credentials completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export git repo credential resources of cluster groups\n",
    "./013-clustergroup-repository-credentials-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b27a88ee-b6f2-4418-bb1f-ee1f6829336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:38:38 INFO  [014] Export the cluster group git repositories ...\n",
      "2025-06-26 06:38:43 INFO  [014] Export the cluster group git repositories completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export git repository resources of cluster groups\n",
    "./014-clustergroup-git-repositories-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f55f426-5c74-40fe-807b-c82b191f70fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:38:52 INFO  [015] Export the cluster group kustomizations ...\n",
      "2025-06-26 06:38:56 INFO  [015] Export the cluster group kustomizations completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export kustomization resources of cluster groups\n",
    "./015-clustergroup-kustomizations-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e956ab88-0094-4df8-a3d0-12b84a588c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:39:04 INFO  [016] Export the cluster group helms ...\n",
      "2025-06-26 06:39:08 INFO  [016] Export the cluster group helms completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export helm resources of cluster groups\n",
    "./016-clustergroup-helms-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b95292d7-23ab-442e-933f-016a1bb008f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:39:12 INFO  [017] Export the cluster group helm releases ...\n",
      "2025-06-26 06:39:17 INFO  [017] Export the cluster group helm releases completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export helm release resources of cluster groups\n",
    "./017-clustergroup-helm-releases-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a172d-4c42-4b8f-991c-8e10052a0e53",
   "metadata": {},
   "source": [
    "## Export add-on resources of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1454c644-0e05-4ccc-a783-20124cf2bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:39:29 INFO  [018] Export the cluster managed namespaces ...\n",
      "2025-06-26 06:39:35 INFO  [018] Export the cluster managed namespaces completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export managed namespace resources of clusters\n",
    "./018-cluster-namespaces-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25c2b113-fff5-4903-959c-4d9a10a5fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:39:41 INFO  [019] Export the cluster secrets ...\n",
      "2025-06-26 06:39:50 INFO  Export secrets of cluster sc-803:testns:wc-01\n",
      "2025-06-26 06:39:54 INFO  Export secrets of cluster attached:attached:wc-02\n",
      "2025-06-26 06:39:58 INFO  [019] Export the cluster secrets completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export k8s secret resources of clusters\n",
    "./019-cluster-secrets-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4e712a8-8ffd-4478-96b8-0e413c72f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:40:09 INFO  [020] Export the cluster secret exports ...\n",
      "2025-06-26 06:40:18 INFO  Export secret exports of cluster sc-803:testns:wc-01\n",
      "2025-06-26 06:40:21 INFO  Export secret exports of cluster attached:attached:wc-02\n",
      "2025-06-26 06:40:25 INFO  [020] Export the cluster secret exports completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export k8s secret export resources of clusters\n",
    "./020-cluster-secret-exports-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec7809d2-954d-4e74-b25e-608060b830db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:40:33 INFO  [021] Export the cluster continuous deliveries ...\n",
      "2025-06-26 06:40:37 INFO  [021] Export the cluster continuous deliveries completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export fluxcd resources of clusters\n",
    "./021-cluster-continuous-deliveries-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dad675f-1cdd-406b-bdf8-3f2af12e583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:41:00 INFO  [022] Export the cluster repository credentials ...\n",
      "2025-06-26 06:41:09 INFO  Export repository credentials of cluster sc-803:testns:wc-01\n",
      "2025-06-26 06:41:14 INFO  Export repository credentials of cluster attached:attached:wc-02\n",
      "2025-06-26 06:41:17 INFO  [022] Export the cluster repository credentials completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export git repo credential resources of clusters\n",
    "./022-cluster-repository-credentials-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32877cfb-b983-4890-b214-a41d4da96054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:41:26 INFO  [023] Export the cluster git repositories ...\n",
      "2025-06-26 06:41:30 INFO  [023] Export the cluster git repositories completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export git repository resources of clusters\n",
    "./023-cluster-git-repositories-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cd8c631-54cd-40ca-b6f1-6a2dc69ee719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:41:34 INFO  [024] Export the cluster kustomizations ...\n",
      "2025-06-26 06:41:38 INFO  [024] Export the cluster kustomizations completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export kustomization resources of clusters\n",
    "./024-cluster-kustomizations-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f141f58-30fe-40db-acfc-d32ee7d69853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:41:43 INFO  [025] Export the cluster helms ...\n",
      "2025-06-26 06:41:47 INFO  [025] Export the cluster helms completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export helm resources of clusters\n",
    "./025-cluster-helms-export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc30fd6f-f119-42e3-aeca-0bc49d884b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 06:41:50 INFO  [026] Export the cluster helm releases ...\n",
      "2025-06-26 06:41:54 INFO  [026] Export the cluster helm releases completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Export helm release resources of clusters\n",
    "./026-cluster-helm-releases-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17f2f6-37e6-411e-8183-b0496214c52e",
   "metadata": {},
   "source": [
    "## Export data protection resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2c2c9c6-63f8-4f39-9bba-78ac99e8913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving backup-location ......\n",
      "Saving schedule for clusters ......\n",
      "Saving schedule for clustergroups ......\n",
      "    clustergroup: default\n",
      "    clustergroup: test-cg1\n",
      "Saving backup ......\n",
      "Saving restore ......\n",
      "Saving others ......\n",
      "Saving dataprotection for clustergroups ......\n",
      "    clustergroup: default\n",
      "    clustergroup: test-cg1\n",
      "Saving dataprotection for clusters ......\n",
      "    cluster: wc-01\n",
      "Removing orgId from files ......\n",
      "data/data-protection/backup_location_cluster.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_cluster.yaml:      orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_cluster.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_cluster.yaml:      orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:        orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:      orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:        orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:      orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:        orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup_location_org.yaml:      orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n",
      "data/data-protection/backup.yaml:    orgId: b5fff09e-221d-4a22-94d5-21533b7581a9\n"
     ]
    }
   ],
   "source": [
    "# Export data-protection\n",
    "./027-cluster-data_protection-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad51429-f067-460e-ae27-eec30b8ce3e4",
   "metadata": {},
   "source": [
    "## Export Polices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed05250-a58a-4494-afef-19ad9aa29535",
   "metadata": {},
   "source": [
    "### ES-28: Export Access Policies from SaaS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b24cc714-4ebe-4809-9d8c-e774cbc6304b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Access Policies from TMC SaaS ...\n",
      "************************************************************************\n",
      "2025-06-26 06:42:48 INFO  Exporting rolebindings on organization ...\n",
      "2025-06-26 06:42:52 INFO  Exporting rolebindings on workspaces ...\n",
      "2025-06-26 06:42:52 INFO  Exporting rolebindings on workspace default ...\n",
      "2025-06-26 06:42:54 INFO  Exporting rolebindings on workspace test-w1 ...\n",
      "2025-06-26 06:42:56 INFO  Exporting rolebindings on workspace test-w2 ...\n",
      "2025-06-26 06:42:59 INFO  Exporting rolebindings on clustergroups ...\n",
      "2025-06-26 06:42:59 INFO  Exporting rolebindings on clustergroup default ...\n",
      "2025-06-26 06:43:01 INFO  Exporting rolebindings on clustergroup test-cg ...\n",
      "2025-06-26 06:43:03 INFO  Exporting rolebindings on clustergroup test-cg1 ...\n",
      "2025-06-26 06:43:06 INFO  Exporting rolebindings on clustergroup test-cg2 ...\n",
      "2025-06-26 06:43:08 INFO  Exporting rolebindings on clusters ...\n",
      "2025-06-26 06:43:12 INFO  Exporting rolebindings on cluster /vks-02/testns/vks-02-cc-01 ...\n",
      "2025-06-26 06:43:14 INFO  Exporting rolebindings on cluster /sc-803/testns/wc-01 ...\n",
      "2025-06-26 06:43:16 INFO  Exporting rolebindings on cluster /attached/attached/wc-02 ...\n",
      "2025-06-26 06:43:18 INFO  Exporting rolebindings on namespaces ...\n",
      "2025-06-26 06:43:18 INFO  Exporting rolebindings on namespace /attached/attached/wc-02/test-ns1 ...\n",
      "2025-06-26 06:43:21 INFO  Exporting rolebindings on namespace /sc-803/testns/wc-01/testns ...\n"
     ]
    }
   ],
   "source": [
    "./028-base-access-policies-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f12c31-324d-45f5-841b-ca8a9c0453f9",
   "metadata": {},
   "source": [
    "### ES-29: Export Policy Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c700eeb-b049-4d78-b069-607f473b4ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Policy Templates from TMC SaaS ...\n",
      "************************************************************************\n",
      "2025-06-26 06:43:38 INFO  Exporting policy templates ...\n"
     ]
    }
   ],
   "source": [
    "./029-base-policy-templates-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525f50b-c736-4f47-a8ed-5e60721fa810",
   "metadata": {},
   "source": [
    "### ES-30: Export Policy Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "834b06de-8b7e-45fa-b358-5c588b10ba2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Exporting Policy Assignments from TMC SaaS ...\n",
      "************************************************************************\n",
      "2025-06-26 06:43:58 INFO  Exporting policies on organization ...\n",
      "2025-06-26 06:44:02 INFO  Exporting policies on workspaces ...\n",
      "2025-06-26 06:44:02 INFO  Exporting policies on workspace default ...\n",
      "2025-06-26 06:44:09 INFO  Exporting policies on workspace test-w1 ...\n",
      "2025-06-26 06:44:17 INFO  Exporting policies on workspace test-w2 ...\n",
      "2025-06-26 06:44:24 INFO  Exporting policies on clustergroups ...\n",
      "2025-06-26 06:44:24 INFO  Exporting policies on clustergroup default ...\n",
      "2025-06-26 06:44:32 INFO  Exporting policies on clustergroup test-cg ...\n",
      "2025-06-26 06:44:39 INFO  Exporting policies on clustergroup test-cg1 ...\n",
      "2025-06-26 06:44:47 INFO  Exporting policies on clustergroup test-cg2 ...\n",
      "2025-06-26 06:44:54 INFO  Exporting policies on clusters ...\n",
      "2025-06-26 06:44:58 INFO  Exporting policies on cluster /vks-02/testns/vks-02-cc-01 ...\n",
      "2025-06-26 06:45:06 INFO  Exporting policies on cluster /sc-803/testns/wc-01 ...\n",
      "2025-06-26 06:45:13 INFO  Exporting policies on cluster /attached/attached/wc-02 ...\n"
     ]
    }
   ],
   "source": [
    "./030-base-policy-assignments-export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e32b0-353e-4872-9285-97a42a87bbc4",
   "metadata": {},
   "source": [
    "## Offboard clusters from SaaS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc8330-2071-4302-8ddf-6cd2c5ccfcc7",
   "metadata": {},
   "source": [
    "### Managed clusters (VKS/TKG)\n",
    "\n",
    "**NOTE:** Set environment variable `TMC_MC_FILTER=\"mc_cluster1,mc_cluster2\"` to export the clusters under the specified management clusters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd11bbc9-c31d-4a4d-a0c8-7a955145ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMC_MC_FILTER=sc-803\n",
      "Management cluster filter TMC_MC_FILTER=sc-803\n",
      "Export management clusters matching pattern sc-803\n",
      "Export the workload clusters under management cluster sc-803 to clusters/wc_of_sc-803.yaml\n",
      "Unmanaging workload clusters under mc sc-803\n",
      "Wait for 1m to ensure all the workload clusters are unmanaged from management cluster sc-803 before deregister it\n",
      "Deregister management cluster sc-803\n",
      "\u001b[92m✔\u001b[0m  successfully de-registered management cluster in TMC \n",
      "\u001b[92m✔\u001b[0m  initiated clean up of TMC resources on cluster \n"
     ]
    }
   ],
   "source": [
    "export TMC_MC_FILTER=\"sc-803\"\n",
    "echo \"TMC_MC_FILTER=$TMC_MC_FILTER\"\n",
    "\n",
    "./031-base-managed_clusters-offboard.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3e5c3-3f3c-4b8c-90f4-e44d676d66ca",
   "metadata": {},
   "source": [
    "### Attached clusters\n",
    "\n",
    "**NOTE:** Set environment variable `CLUSTER_NAME_FILTER=\"cluster1,cluster2\"` to export the specified attached clusters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2720fa41-beb7-47d8-81d4-70b74bf4150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER_NAME_FILTER=wc-02\n",
      "Export all the non-NPC attached clusters\n",
      "Filter clusters with filter CLUSTER_NAME_FILTER=wc-02\n",
      "Detaching healthy cluster wc-02 normally\n",
      "\u001b[36mℹ\u001b[0m  Deleting cluster... \n",
      "\u001b[92m✔\u001b[0m  Cluster is being detached \n"
     ]
    }
   ],
   "source": [
    "export CLUSTER_NAME_FILTER=\"wc-02\"\n",
    "echo \"CLUSTER_NAME_FILTER=$CLUSTER_NAME_FILTER\"\n",
    "\n",
    "./032-base-attached_non_npc_clusters-offboard.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d828cc-5e21-4133-b628-5cd4b75a7556",
   "metadata": {},
   "source": [
    "**NOTE: Cluster unmanage/detach/deregister may need a bit more time, even if the command is returned. Suggest double-checking from the SaaS stack to ensure all the clusters are offboarded before proceeding to the next steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb33247-117b-4a67-b436-160094a6a446",
   "metadata": {},
   "source": [
    "## Connect to the TMC SM stack to onboard clusters and restore resources\n",
    "\n",
    "**NOTE:** Set the necessary environment variables to initialize the CLI context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f64e60ff-56c1-4010-8a26-1a20c93fbf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export environment variables for connecting SM stack\n",
      "TMC_SELF_MANAGED_USERNAME=testuser01@tmcselfmanaged.com\n",
      "TMC_SELF_MANAGED_PASSWORD=VMware1!\n",
      "TMC_SELF_MANAGED_DNS=tmc.tanzu.io\n",
      "TMC_SM_CONTEXT=tmc-sm\n",
      "Run 033-base-sm_stack-connect.sh to init CLI context for SM stack\n",
      "[ok] Successfully deleted context \"tmc-sm\"\n",
      "[i] Starting TMC login...\n",
      "[i] IDP type is set to \"pinniped\"\n",
      "[i] endpoint is set to \"tmc.tanzu.io\"\n",
      "[i] [i] context is set to \"tmc-sm\"\n",
      "[i] Logging in to TMC Self Managed (Pinniped)...\n",
      "[i] Pinniped Username/Password login...\n",
      "[i] Successfully created a TMC SelfManaged (Pinniped) context, \"tmc-sm\"\n",
      "[i] Synchronizing other TMC plugins...\n",
      "[i] Fetching recommended plugins for active context 'tmc-sm'...\n",
      "[ok] All recommended plugins are already installed and up-to-date.\n"
     ]
    }
   ],
   "source": [
    "echo \"Export environment variables for connecting SM stack\"\n",
    "\n",
    "export TMC_SELF_MANAGED_USERNAME=testuser01@tmcselfmanaged.com\n",
    "export TMC_SELF_MANAGED_PASSWORD=VMware1!\n",
    "export TMC_SELF_MANAGED_DNS=tmc.tanzu.io\n",
    "export TMC_SM_CONTEXT=tmc-sm\n",
    "\n",
    "echo \"TMC_SELF_MANAGED_USERNAME=$TMC_SELF_MANAGED_USERNAME\"\n",
    "echo \"TMC_SELF_MANAGED_PASSWORD=$TMC_SELF_MANAGED_PASSWORD\"\n",
    "echo \"TMC_SELF_MANAGED_DNS=$TMC_SELF_MANAGED_DNS\"\n",
    "echo \"TMC_SM_CONTEXT=$TMC_SM_CONTEXT\"\n",
    "\n",
    "echo \"Run 033-base-sm_stack-connect.sh to init CLI context for SM stack\"\n",
    "./033-base-sm_stack-connect.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "148e72a5-bb1e-4602-af21-35a3de024a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name:        tmc-sm\n",
      "  Type:        mission-control\n"
     ]
    }
   ],
   "source": [
    "tanzu context current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66afe0e-ed66-42d7-9bc7-aeecabdbcbeb",
   "metadata": {},
   "source": [
    "## Import resources PRE cluster onboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279be4f-5122-4b73-b741-99349ec7329c",
   "metadata": {},
   "source": [
    "### Import base resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7ec122c-a301-47c2-8194-b554f44e1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Cluster Groups into TMC SM ...\n",
      "************************************************************************\n",
      "Create clustergroup: test-cg\n",
      "\u001b[32m√\u001b[0m ClusterGroup \"test-cg\" is being created\n",
      "Create clustergroup: test-cg1\n",
      "\u001b[32m√\u001b[0m ClusterGroup \"test-cg1\" is being created\n",
      "Create clustergroup: test-cg2\n",
      "\u001b[32m√\u001b[0m ClusterGroup \"test-cg2\" is being created\n",
      "Imported Cluster Groups into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Cluster Groups\n",
    "./034-base-clustergroups-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "936add6d-5e97-48bf-838b-15eb7a8522db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Workspaces into TMC SM ...\n",
      "************************************************************************\n",
      "Create workspace - test-w1\n",
      "\u001b[32m√\u001b[0m workspace \"test-w1\" is being created\n",
      "Create workspace - test-w2\n",
      "\u001b[32m√\u001b[0m workspace \"test-w2\" is being created\n",
      "Imported Workspaces into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Workspaces\n",
    "./035-base-workspaces-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630ce4b-56e6-4037-89e4-adcd1a166b85",
   "metadata": {},
   "source": [
    "### Import administration resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d35dd27c-df14-4d9d-bf57-a8185505b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Customized Roles into TMC SM ...\n",
      "************************************************************************\n",
      "Create role my-cus-role01\n",
      "\u001b[92m✔\u001b[0m  role \"my-cus-role01\" is being created\n",
      "Create role my-cus-role02\n",
      "\u001b[92m✔\u001b[0m  role \"my-cus-role02\" is being created\n",
      "Create role my-cus-role-dep\n",
      "\u001b[92m✔\u001b[0m  role \"my-cus-role-dep\" is being created\n",
      "Imported Customized Roles into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Roles\n",
    "./036-admin-roles-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bba04b5d-966e-486d-9e21-c0cccabf9223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate template yaml files for credentials with script 037-admin-credentials-create-template.sh\n",
      "\n",
      "Template examples:\n",
      "\n",
      "1.Spec Format for Self-provisioned: AWS S3 or S3 compatible\n",
      "##################################################################\n",
      "spec:\n",
      "  capability: DATA_PROTECTION\n",
      "  data:\n",
      "    keyValue:\n",
      "      data:\n",
      "        aws_access_key_id: \"<Your aws_access_key_id>\"\n",
      "        aws_secret_access_key: \"<Your aws_secret_access_key>\"\n",
      "      type: SECRET_TYPE_UNSPECIFIED\n",
      "  meta:\n",
      "    provider: GENERIC_S3\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "2.Spec Format for Self-provisioned: Azure Blob\n",
      "##################################################################\n",
      "\n",
      "spec:\n",
      "  capability: DATA_PROTECTION\n",
      "  data:\n",
      "    azureCredential:\n",
      "      servicePrincipal:\n",
      "        azureCloudName: <AzurePublicCloud | AzureUSGovernmentCloud | AzureChinaCloud | AzureGermanCloud>\n",
      "        clientId: <Your clientId>\n",
      "        clientSecret: <Your clientSecret>\n",
      "        resourceGroup: <Your resource group>\n",
      "        subscriptionId: <Your subscriptionId>\n",
      "        tenantId: <Your tenantId>\n",
      "  meta:\n",
      "    provider: AZURE_AD\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "3.Spec Format for Self-provisioned: AWS_EC2\n",
      "##################################################################\n",
      "spec:\n",
      "  capability: DATA_PROTECTION\n",
      "  data:\n",
      "    awsCredential:\n",
      "      accountId: \"<Your accountId or empty string>\"\n",
      "      iamRole:\n",
      "        arn: \"<Your arn>\"\n",
      "        extId: \"<Your extId>\"\n",
      "  meta:\n",
      "    provider: AWS_EC2\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "##################################################################\n",
      "You need to go to the dir: data/credential/template to fill the missing fields for each template file before execute the import script.\n"
     ]
    }
   ],
   "source": [
    "# Generate template yaml files for each credential And User need to fill in the missing values such as Crendentials and CA\n",
    "./037-admin-credentials-create-template.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "201ace4f-8f2e-45b8-aa65-c6d7834be62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Credentials into TMC SM ...\n",
      "************************************************************************\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/AWS_EC2--aws-s3-test.yaml\n",
      "\u001b[31mx\u001b[0m rpc error: code = InvalidArgument desc = credential aws-s3-test is not currently supported, only customer managed credentials are enabled \n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--dp-for-migration.yaml\n",
      "\u001b[32m√\u001b[0m credential \"dp-for-migration\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--dp-for-migration.yaml\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--hxie-minio-acct.yaml\n",
      "\u001b[32m√\u001b[0m credential \"hxie-minio-acct\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--hxie-minio-acct.yaml\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--test2.yaml\n",
      "\u001b[32m√\u001b[0m credential \"test2\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--test2.yaml\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--test.yaml\n",
      "\u001b[32m√\u001b[0m credential \"test\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/credential/template/GENERIC_S3--test.yaml\n",
      "Imported Credentials into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import credentials\n",
    "./037-admin-credentials-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b3b072f-4de9-4273-84c6-a759deaeb806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate proxy configuration template yaml files\n",
      "\n",
      "Template examples:\n",
      "\n",
      "1.Spec Format for Proxy\n",
      "##################################################################\n",
      "spec:\n",
      "  capability: PROXY_CONFIG\n",
      "  data:\n",
      "    keyValue:\n",
      "      data:\n",
      "        httpUserName: \"<base64 string>\"\n",
      "        httpPassword: \"<base64 string>\"\n",
      "        httpsUserName: \"<base64 string>\"\n",
      "        httpsPassword: \"<base64 string>\"\n",
      "        proxyCABundle: \"<base64 string>\"\n",
      "      type: SECRET_TYPE_UNSPECIFIED\n",
      "  meta:\n",
      "    provider: PROVIDER_UNSPECIFIED\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "##################################################################\n",
      "The generated template files are without credentials.\n",
      "You need to go to the dir: data/proxy/template to fill the missing field values for each template file before execute the import script.\n"
     ]
    }
   ],
   "source": [
    "# Generate template yaml files for each proxy And User need to fill in the missing values such as Crendentials and CA\n",
    "./038-admin-proxy-create-template.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ee08056-7fbb-4f6d-9044-77a159d692f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Proxy into TMC SM ...\n",
      "************************************************************************\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/proxy-01.yaml\n",
      "\u001b[32m√\u001b[0m credential \"proxy-01\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/proxy-01.yaml\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/proxy-02.yaml\n",
      "\u001b[32m√\u001b[0m credential \"proxy-02\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/proxy-02.yaml\n",
      "Create credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/w2-ca.yaml\n",
      "\u001b[32m√\u001b[0m credential \"w2-ca\" is being created\n",
      "Successfully created the credential with file /home/kubo/tmc-saas-migration-scripts/data/proxy/template/w2-ca.yaml\n",
      "Imported Proxy into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Proxy Configuration\n",
    "./038-admin-proxy-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "765d3461-3be0-4046-96e6-0e657b22be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate image registry template yaml files\n",
      "\n",
      "Template examples:\n",
      "\n",
      "1.Spec Format for Image registry without username and password\n",
      "##################################################################\n",
      "spec:\n",
      "  capability: IMAGE_REGISTRY\n",
      "  data:\n",
      "    keyValue:\n",
      "      data:\n",
      "        registry-url: <registry-url in base64 string>\n",
      "  meta:\n",
      "    provider: GENERIC_KEY_VALUE\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "\n",
      "2.Spec Format for Image registry with username and password\n",
      "##################################################################\n",
      "spec:\n",
      "  capability: IMAGE_REGISTRY\n",
      "  data:\n",
      "    keyValue:\n",
      "      data:\n",
      "        .dockerconfigjson: \"<base64 string or call ./utils/create-docker-config-json-base64.sh to generate base64 string>\"\n",
      "        ca-cert: \"<base64 string or remove key/value if not needed >\"\n",
      "      type: DOCKERCONFIGJSON_SECRET_TYPE\n",
      "  meta:\n",
      "    provider: GENERIC_KEY_VALUE\n",
      "    temporaryCredentialSupport: false\n",
      "\n",
      "##################################################################\n",
      "The generated template files are without credentials.\n",
      "You need to go to the dir: data/image-registry/template to fill the missing field values for each template file before execute the import script.\n"
     ]
    }
   ],
   "source": [
    "# Generate template yaml files for each Image-Registry And User need to fill in the missing values such as Crendentials and CA\n",
    "./039-admin-image-registry-create-template.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "294f8465-2e8c-4b3b-9f54-18b2655d4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Image Registry into TMC SM ...\n",
      "************************************************************************\n",
      "Create image registry with file /home/kubo/tmc-saas-migration-scripts/data/image-registry/template/my-lir-test.yaml\n",
      "\u001b[32m√\u001b[0m credential \"my-lir-test\" is being created\n",
      "Successfully created the image registry with file /home/kubo/tmc-saas-migration-scripts/data/image-registry/template/my-lir-test.yaml\n",
      "Imported Image Registry into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Image-Registry\n",
    "./039-admin-image-registry-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f94d4-673b-4d66-81bb-d1cbcb604d02",
   "metadata": {},
   "source": [
    "### Import add-on resources of cluster groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be51481b-0fe3-41fa-b04a-7e4aecea3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:35:37 INFO  [040] Import the cluster group secrets ...\n",
      "2025-06-26 07:35:39 INFO  Import secret 'web-postgres-creds' in namespace 'default' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:35:39 INFO  Import secret 'web-postgres-creds' in namespace 'default' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:35:39 INFO  [040] Import the cluster group secrets completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import k8s secret resources to cluster groups\n",
    "./040-clustergroup-secrets-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf71b8-31d3-46fd-bcfb-f3491c5b94ee",
   "metadata": {},
   "source": [
    "**USER ACTION REQUIRED**\n",
    "\n",
    "Next step is importing k8s secret resource to the cluster groups\n",
    "\n",
    "User must manually fill in the missing data fields depending on the type of k8s secret into the manifests in directory `./data/clustergroup-secrets`\n",
    "\n",
    "* SECRET_TYPE_OPAQUE\n",
    "  ```yaml\n",
    "  spec:\n",
    "    atomicSpec:\n",
    "      data: # filled data field\n",
    "        key1: base64-encoded-value1\n",
    "        key2: base64-encoded-value2\n",
    "      secretType: SECRET_TYPE_OPAQUE\n",
    "  ```\n",
    "* SECRET_TYPE_DOCKERCONFIGJSON\n",
    "  ```yaml\n",
    "  spec:\n",
    "    atomicSpec:\n",
    "      data: # filled data field\n",
    "        .dockerconfigjson: base64-encoded-dockerconfig-json-file\n",
    "      secretType: SECRET_TYPE_DOCKERCONFIGJSON\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bc648ec-197b-4357-a2db-a7139dccc75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:35:49 INFO  [041] Import the cluster group secret exports ...\n",
      "2025-06-26 07:35:50 INFO  Import secretexport 'web-postgres-creds' in namespace 'default' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:35:50 INFO  [041] Import the cluster group secret exports completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import k8s secret export resources to cluster groups\n",
    "./041-clustergroup-secret-exports-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7743b42-7ce9-42fc-9f79-cce16f49e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:36:19 INFO  [042] Import the cluster group continuous deliveries ...\n",
      "\u001b[92m✔\u001b[0m  continuous delivery is being enabled \n",
      "2025-06-26 07:36:20 INFO  Import continuousdelivery to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:36:20 INFO  [042] Import the cluster group continuous deliveries completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import fluxcd resources to cluster groups\n",
    "./042-clustergroup-continuous-deliveries-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39767bb-ca83-47bc-97cf-b3d1a8c6a376",
   "metadata": {},
   "source": [
    "**USER ACTION REQUIRED**\n",
    "\n",
    "Next step is importing git repository credential resources to the cluster groups\n",
    "\n",
    "Users must manually fill in the missing data field depending on the type of credential into the manifests in directory `./data/clustergroup-repository-credentials`\n",
    "\n",
    "* Username/Password\n",
    "  ```yaml\n",
    "  spec:\n",
    "    atomicSpec:\n",
    "      data: # filled data field\n",
    "        data:\n",
    "          username: bas64-encoded-username\n",
    "          password: base64-encoded-password\n",
    "      sourceSecretType: USERNAME_PASSWORD\n",
    "  ```\n",
    "* SSH Authentication\n",
    "  ```yaml\n",
    "  spec:\n",
    "    atomicSpec:\n",
    "      data: # filled data field\n",
    "        data:\n",
    "          identity: bas64-encoded-ssh-identity\n",
    "          known_hosts: base64-encoded-ssh-known-hosts\n",
    "      sourceSecretType: SSH\n",
    "  ```\n",
    "* CA Certificate\n",
    "  ```yaml\n",
    "  spec:\n",
    "    atomicSpec:\n",
    "      data: # filled data field\n",
    "        data:\n",
    "          ca.crt: bas64-encoded-ca-crt\n",
    "          username: base64-encoded-username  # username and password are optional\n",
    "          password: base64-encoded-password  # username and password are optional\n",
    "      sourceSecretType: CACert\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8d75c0e-bc7a-4e73-b6df-b149dd938f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:41:36 INFO  [043] Import the cluster group repository credentials ...\n",
      "2025-06-26 07:41:39 INFO  Import sourcesecret 'github-deploy-key' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:41:39 INFO  Import sourcesecret 'github-deploy-key' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:41:39 INFO  [043] Import the cluster group repository credentials completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import git repository credential resources to cluster groups\n",
    "./043-clustergroup-repository-credentials-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d971bfa0-10c9-40a4-8d36-26a84191f517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:41:47 INFO  [044] Import the cluster group git repositories ...\n",
      "2025-06-26 07:41:48 INFO  Import gitrepository 'podinfo' in namespace 'tanzu-continuousdelivery-resources' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:41:48 INFO  [044] Import the cluster group git repositories completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import git repository resources to cluster groups\n",
    "./044-clustergroup-git-repositories-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9bfb9c1b-e464-4014-a4d8-c850bed2f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:41:52 INFO  [045] Import the cluster group kustomizations ...\n",
      "2025-06-26 07:41:53 INFO  Import kustomization 'podinfo' in namespace 'tanzu-continuousdelivery-resources' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:41:53 INFO  [045] Import the cluster group kustomizations completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import kustomization resources to cluster groups\n",
    "./045-clustergroup-kustomizations-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "806ca859-4eca-4942-bfef-ff72d07358c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:41:58 INFO  [046] Import the cluster group helms ...\n",
      "\u001b[92m✔\u001b[0m  helm feature is being enabled for a cluster group \n",
      "2025-06-26 07:41:59 INFO  Import helm to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:41:59 INFO  [046] Import the cluster group helms completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import helm resources to cluster groups\n",
    "./046-clustergroup-helms-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2357bad9-d78d-4bf4-862d-80d789539975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 07:42:06 INFO  [047] Import the cluster group helm releases ...\n",
      "2025-06-26 07:42:07 INFO  Import release 'podinfo' in namespace 'tanzu-helm-resources' to cluster group 'test-cg' successfully\n",
      "2025-06-26 07:42:07 INFO  [047] Import the cluster group helm releases completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import helm release resources to cluster groups\n",
    "./047-clustergroup-helm-releases-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403013c7-72a7-47bf-a40d-f51ab72f2959",
   "metadata": {},
   "source": [
    "## Onboard the exported clusters to the SM stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304515d4-198d-4ac5-b687-c29329cc1af3",
   "metadata": {},
   "source": [
    "### Managed clusters (VKS/TKGM)\n",
    "\n",
    "Run script `048-prepare-for-user-input.sh` to generate a placeholder kubeconfig index file of exported management clusters.\n",
    "\n",
    "**NOTE**: Update the generated kubeconfig index file `clusters/mc-kubeconfig-index-file` by replacing the placeholder text '/path/to/the/real/mc_kubeconfig/file' with the real kubeconfig file of the onboarding management clusters before moving to the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "674caad5-3666-4bbd-9c38-56b396a9e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append management cluster sc-803 to data/clusters/mc-kubeconfig-index-file\n",
      "sc-803: /path/to/the/real/mc_kubeconfig/file\n"
     ]
    }
   ],
   "source": [
    "./048-prepare-for-user-input.sh\n",
    "cat data/clusters/mc-kubeconfig-index-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ea0d5-a4ce-4c7d-91ce-fe99eca1977d",
   "metadata": {},
   "source": [
    "**NOTE:** If the onboarding clusters are exported without proxy settings, but proxy settings are needed for SM, modify the cluster configurations under `data/clusters/wc_of_{{MANAGEMENT-CLUSTER-NAME}}.yaml`.\n",
    "If the onboarding clusters are exported with proxy settings, they will be configured with the same proxy settings that have been imported in the previous step.\n",
    "\n",
    "Modify:\n",
    "```yaml\n",
    "spec:\n",
    "  proxyName: <YOUR-PROXY-ON-SM>\n",
    "```\n",
    "\n",
    "**Run managed cluster onboard script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "958b27a8-343d-4c1b-b504-2cf97574af95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mc sc-803 [health=HEALTHY]\n",
      "Checking namespace: svc-tmc-c9\n",
      "🧹 Deleting AgentInstall(s) in svc-tmc-c9...\n",
      "No resources found\n",
      "Apply uninstall operation\n",
      "agentinstall.installers.tmc.cloud.vmware.com/tmc-agent-installer-config created\n",
      "Waiting for all pods in namespace 'svc-tmc-c9' to terminate...\n",
      "All pods in namespace 'svc-tmc-c9' are gone.\n",
      "🧹 Deleting AgentInstall(s) in svc-tmc-c9...\n",
      "agentinstall.installers.tmc.cloud.vmware.com \"tmc-agent-installer-config\" deleted\n",
      "Deleting AgentConfig(s) in svc-tmc-c9...\n",
      "No resources found\n",
      "agentconfig.installers.tmc.cloud.vmware.com/tmc-agent-config created\n",
      "\u001b[92m✔\u001b[0m  management cluster \"sc-803\" is being created\n",
      "\u001b[36mℹ\u001b[0m  identifying TMC namespace \n",
      "\u001b[36mℹ\u001b[0m  installing TMC agents on the management cluster \n",
      "\u001b[37m└\u001b[0mError: failed to install TMC agents: timed out waiting for TMC Agent installation\n",
      "\u001b[1mUsage:\u001b[0m\n",
      "  tanzu mission-control management-cluster register MANAGEMENT_CLUSTER_NAME [flags]\n",
      "\n",
      "\u001b[1mFlags:\u001b[0m\n",
      "      --continue-bootstrap                               continue registration to apply the Tanzu Mission Control resource manifest to complete registration\n",
      "  -v, --data-values-file string                          data values file to use\n",
      "  -c, --default-cluster-group string                     default cluster group for workload clusters\n",
      "      --default-workload-cluster-image-registry string   Default workload cluster image registry configuration. If set empty, no image registry config will be used. If left unset, management cluster's image registry config will be used (default \"use-default\")\n",
      "      --default-workload-cluster-proxy-name string       default workload cluster proxy configuration\n",
      "  -f, --file string                                      Resource file from which to create a management cluster entry\n",
      "  -h, --help                                             help for register\n",
      "      --image-registry string                            Image registry configuration to be used for the management cluster\n",
      "  -k, --kubeconfig string                                the kubeconfig file of the management cluster to use for applying the Tanzu Mission Control resource manifest\n",
      "  -p, --kubernetes-provider-type string                  indicates the k8s provider type, supported values are 'TKGS' 'TKG' and 'TCE' (default \"TKGS\")\n",
      "  -n, --name string                                      name of the management cluster. Used in conjunction with --continue-bootstrap flag\n",
      "  -o, --output string                                    output file to write the Tanzu Mission Control resource manifest for registering a cluster (default \"k8s-register-manifest.yaml\")\n",
      "      --proxy-name string                                proxy configuration to be used for the management cluster\n",
      "      --skip-verify                                      skip the post-flight checks to confirm that the cluster registration was successful\n",
      "  -t, --template string                                  template to use (default \"default\")\n",
      "\n",
      "Run tanzu tmc mc wc manage \"wc-01\" -m \"sc-803\" -p \"testns\" --cluster-group \"default\" --proxy-name \"w2-ca\"\n",
      "Error: rpc error: code = NotFound desc = cluster not found wc-01\n",
      "\u001b[1mUsage:\u001b[0m\n",
      "  tanzu mission-control management-cluster workload-cluster manage CLUSTER_NAME [flags]\n",
      "\n",
      "\u001b[1mFlags:\u001b[0m\n",
      "      --cluster-group string             target clustergroup to add the workload cluster\n",
      "  -h, --help                             help for manage\n",
      "      --image-registry string            The image registry name to use with this workload cluster\n",
      "  -m, --management-cluster-name string   name of the management cluster\n",
      "  -p, --provisioner-name string          provisioner name of the cluster\n",
      "      --proxy-name string                The proxy config name to use with this workload cluster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "./048-base-managed_clusters-onboard.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad95668-0a50-4ac9-a2c5-a3e87319490b",
   "metadata": {},
   "source": [
    "### Attached clusters\n",
    "\n",
    "Run script `049-prepare-for-user-input.sh` to generate a placeholder kubeconfig index file of exported attached clusters\n",
    "\n",
    "**NOTE**: Update the generated kubeconfig index file `clusters/attached-wc-kubeconfig-index-file` by replacing the placeholder text '/path/to/the/real/wc_kubeconfig/file' with the real kubeconfig file of the onboarding attached clusters before moving to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4dd15173-f843-4205-a60e-1f3a809f310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append cluster wc-02 to data/clusters/attached-wc-kubeconfig-index-file\n",
      "wc-02: /path/to/the/real/wc_kubeconfig/file\n"
     ]
    }
   ],
   "source": [
    "./049-prepare-for-user-input.sh\n",
    "cat data/clusters/attached-wc-kubeconfig-index-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a6eb5-fdba-4a5f-9098-91b0b21c7541",
   "metadata": {},
   "source": [
    "**NOTE:** If the onboarding clusters are exported without proxy settings, but proxy settings are needed for SM, modify the cluster configurations under `clusters/attached_non_npc_clusters.yaml`.\n",
    "If the onboarding clusters are exported with proxy settings, they will be configured with the same proxy settings that have been imported in the previous step.\n",
    "\n",
    "Modify:\n",
    "```yaml\n",
    "spec:\n",
    "  proxyName: <YOUR-PROXY-ON-SM>\n",
    "```\n",
    "\n",
    "**Run the attached cluster onboard script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3bce4c95-8825-495a-8714-c003d67b2b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching cluster 'wc-02' with kubeconfig '/home/kubo/kubeconfigs/wc-02.kubeconfig'\n",
      "\u001b[36mℹ\u001b[0m  running pre-flight checks \n",
      "\u001b[92m✔\u001b[0m  cluster \"wc-02\" is being created\n",
      "\u001b[37m┌\u001b[0m\u001b[32m√\u001b[0m TMC resources applied to the cluster successfully \n",
      "\u001b[K\u001b[36mℹ\u001b[0m  running post-flight attach checks \n",
      "\u001b[K\u001b[92m✔\u001b[0m  cluster wc-02 successfully attached to TMC\n",
      "grep: wc-02: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "./049-base-non_npc_clusters-onboard.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21636e3f-8e30-4784-8778-37e7e7b1c588",
   "metadata": {},
   "source": [
    "## Import resources POST cluster onboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabacf8c-d138-4266-8615-051ed271cd28",
   "metadata": {},
   "source": [
    "### Import add-on resources of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "920dc014-007a-4829-b774-f097b06b29df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:40:02 INFO  [050] Import the managed namespaces ...\n",
      "2025-06-26 08:40:04 INFO  Import namespace 'test-ns1' to cluster 'attached:attachedwc-02' successfully\n",
      "2025-06-26 08:40:05 INFO  Import namespace 'testns' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:40:05 INFO  [050] Import the managed namespaces completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import managed namespace resources to clusters\n",
    "./050-cluster-namespaces-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd08bee-1934-44bd-8e03-d14a9e1c5696",
   "metadata": {},
   "source": [
    "**USER ACTION REQUIRED**\n",
    "\n",
    "Next step is importing k8s secret resources to the clusters\n",
    "\n",
    "Users must manually fill in the missing data field depending on the type of k8s secret into the manifests in directory `./data/cluster-secrets`\n",
    "\n",
    "* SECRET_TYPE_OPAQUE\n",
    "  ```yaml\n",
    "  spec:\n",
    "    data: # filled data field\n",
    "      key1: base64-encoded-value1\n",
    "      key2: base64-encoded-value2\n",
    "    secretType: SECRET_TYPE_OPAQUE\n",
    "  ```\n",
    "* SECRET_TYPE_DOCKERCONFIGJSON\n",
    "  ```yaml\n",
    "  spec:\n",
    "    data: # filled data field\n",
    "      .dockerconfigjson: base64-encoded-dockerconfig-json-file\n",
    "    secretType: SECRET_TYPE_DOCKERCONFIGJSON\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b5ad7b49-bcad-45c7-aa07-29a8c32532f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:40:25 INFO  [051] Import the cluster secrets ...\n",
      "2025-06-26 08:40:26 INFO  Import secret 'web-postgres-creds' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:40:27 INFO  Import secret 'web-postgres-creds' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:40:27 INFO  [051] Import the cluster secrets completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import k8s secret resources to clusters\n",
    "./051-cluster-secrets-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12dda37c-351e-4e3a-931a-5ae1d32a592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:40:34 INFO  [052] Import the cluster secret exports ...\n",
      "2025-06-26 08:40:36 INFO  Import secretexport 'web-postgres-creds' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:40:36 INFO  [052] Import the cluster secret exports completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import k8s secret export resources to clusters\n",
    "./052-cluster-secret-exports-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07430532-64d1-4b9e-97f3-f77c2be50181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:40:55 INFO  [053] Import the cluster continuous deliveries ...\n",
      "\u001b[92m✔\u001b[0m  continuous delivery is being enabled \n",
      "2025-06-26 08:40:56 INFO  Import continuousdelivery to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:40:56 INFO  [053] Import the cluster continuous deliveries completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import fluxcd resources to clusters\n",
    "./053-cluster-continuous-deliveries-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc122e6-2b8a-4759-94bf-27f024236482",
   "metadata": {},
   "source": [
    "**USER ACTION REQUIRED**\n",
    "\n",
    "Next step is importing git repository credential resources to the clusters\n",
    "\n",
    "Users must manually fill in the missing data field depending on the type of credential into the manifests in directory `./data/cluster-repository-credentials`\n",
    "\n",
    "* Username/Password\n",
    "  ```yaml\n",
    "  spec:\n",
    "    data: # filled data field\n",
    "      data:\n",
    "        username: bas64-encoded-username\n",
    "        password: base64-encoded-password\n",
    "    sourceSecretType: USERNAME_PASSWORD\n",
    "  ```\n",
    "* SSH Authentication\n",
    "  ```yaml\n",
    "  spec:\n",
    "    data: # filled data field\n",
    "      data:\n",
    "        identity: bas64-encoded-ssh-identity\n",
    "        known_hosts: base64-encoded-ssh-known-hosts\n",
    "    sourceSecretType: SSH\n",
    "  ```\n",
    "* CA Certificate\n",
    "  ```yaml\n",
    "  spec:\n",
    "    data: # filled data field\n",
    "      data:\n",
    "        ca.crt: bas64-encoded-ca-crt\n",
    "        username: base64-encoded-username  # username and password are optional\n",
    "        password: base64-encoded-password  # username and password are optional\n",
    "    sourceSecretType: CACert\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b7ab30f-65a3-435f-ae54-267918bdb01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:41:16 INFO  [054] Import the cluster repository credentials ...\n",
      "2025-06-26 08:41:17 INFO  Import sourcesecret 'github-deploy-key' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:17 INFO  Import sourcesecret 'github-deploy-key' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:17 INFO  [054] Import the cluster repository credentials completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import git repository credential resources to clusters\n",
    "./054-cluster-repository-credentials-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0383e1e3-b537-4815-82ea-c302b37e5372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:41:22 INFO  [055] Import the cluster git repositories ...\n",
      "2025-06-26 08:41:23 INFO  Import gitrepository 'podinfo' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:23 INFO  [055] Import the cluster git repositories completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import git repository resources to clusters\n",
    "./055-cluster-git-repositories-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06e8f65e-3efc-46a9-9a66-9ee9ff7493ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:41:26 INFO  [056] Import the cluster kustomizations ...\n",
      "2025-06-26 08:41:27 INFO  Import kustomization 'podinfo' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:28 INFO  Import kustomization 'podinfo' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:28 INFO  [056] Import the cluster kustomizations completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import kustomization resources to clusters\n",
    "./056-cluster-kustomizations-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa44b688-2b5b-429e-a7dc-78e1de156ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:41:32 INFO  [057] Import the cluster helms ...\n",
      "\u001b[92m✔\u001b[0m  helm feature is being enabled \n",
      "2025-06-26 08:41:32 INFO  Import helm to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:32 INFO  [057] Import the cluster helms completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import helm resources to clusters\n",
    "./057-cluster-helms-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "141f6e1c-687b-404b-ace1-0088a1c004cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-26 08:41:37 INFO  [058] Import the cluster helm releases ...\n",
      "2025-06-26 08:41:38 INFO  Import helm release 'podinfo' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:38 INFO  Import helm release 'apache' to cluster 'sc-803:testnswc-01' successfully\n",
      "2025-06-26 08:41:38 INFO  [058] Import the cluster helm releases completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import helm releases resources to clusters\n",
    "./058-cluster-helm-releases-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299106c1-d72e-4a5b-8e6c-f71051b02273",
   "metadata": {},
   "source": [
    "### Import administration resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd3bab2b-867d-4695-a0d9-c39d7aa80677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Admin Settings into TMC SM ...\n",
      "************************************************************************\n",
      "No any cluster level settings\n",
      "No any clustergroup level settings\n",
      "No any organization level settings\n",
      "Imported Admin Settings into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Admin Settings\n",
    "./059-admin-settings-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3b700c8-0433-49bb-a7c9-ae7a09bdc5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Importing Admin Access into TMC SM ...\n",
      "************************************************************************\n",
      "Start to check access---aws-s3-test\n",
      "Ignore built-in role access---aws-s3-test\n",
      "Start to check access---dp-for-migration\n",
      "Ignore built-in role access---dp-for-migration\n",
      "Start to check access---hxie-minio-acct\n",
      "Ignore built-in role access---hxie-minio-acct\n",
      "Start to check access---my-lir-test\n",
      "Ignore built-in role access---my-lir-test\n",
      "Start to check access---proxy-01\n",
      "Ignore built-in role access---proxy-01\n",
      "Start to check access---proxy-02\n",
      "Ignore built-in role access---proxy-02\n",
      "Start to check access---test2\n",
      "Ignore built-in role access---test2\n",
      "Start to check access---test\n",
      "Start to create access---test\n",
      "{\"policy\":{\"meta\":{\"uid\":\"cred:01JYNJ80W6RYED2ECEJKAMCBWJ\",\"parentReferences\":[{\"rid\":\"rid:cred:5d805fac-dfeb-48cd-aad2-c0055438f880:test\",\"uid\":\"cred:01JYNJ80W6RYED2ECEJKAMCBWJ\"}]},\"roleBindings\":[{\"role\":\"credential.admin\",\"subjects\":[{\"name\":\"xuel@vmware.com\",\"kind\":\"USER\"}]}]}}Successfully created role binding for credential /home/kubo/tmc-saas-migration-scripts/data/credential-access/access---test.yaml\n",
      "Start to check access---w2-ca\n",
      "Ignore built-in role access---w2-ca\n",
      "Imported Admin Access into TMC SM ...\n"
     ]
    }
   ],
   "source": [
    "# Import Admin Access\n",
    "./060-admin-access-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902f7d6-9ddc-4177-adbc-f5c5dcc18d5c",
   "metadata": {},
   "source": [
    "### Import Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bf230-bb7c-476f-a32d-9abe08f98bb5",
   "metadata": {},
   "source": [
    "#### ES-61: Import Access Policies\n",
    "**Note**: You should edit the user and usergroup identities according to the idP in TMC SM after imported access policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3707cfc-1275-4e90-92c2-c520d5a982b7",
   "metadata": {},
   "source": [
    "* Import Access Policies on Organization, Clustergroups, Workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "07cbcc01-ac2f-4c36-9cbc-f302ff2031a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Import Policy Access to TMC SM ...\n",
      "************************************************************************\n",
      "2025-06-26 10:12:00 INFO  Importing rolebindings on organization ...\n",
      "{\"policy\":{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}}2025-06-26 10:12:02 INFO  Importing rolebindings on clustergroups ...\n",
      "{\"policyList\":[{\"meta\":{\"uid\":\"cg:01JYEJ5FX085GQNV59JZ84NXYT\",\"parentReferences\":[{\"rid\":\"rid:cg:5d805fac-dfeb-48cd-aad2-c0055438f880:default\",\"uid\":\"cg:01JYEJ5FX085GQNV59JZ84NXYT\"}]},\"roleBindings\":[{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"clustergroup.edit\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]}]},{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}]}{\"policyList\":[{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}]}2025-06-26 10:12:05 INFO  [SKIP] no direct rolebinding for clustergroups:test-cg2 is required to imported\n",
      "{\"policyList\":[{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}]}2025-06-26 10:12:07 INFO  Importing rolebindings on workspaces ...\n",
      "{\"policyList\":[{\"meta\":{\"uid\":\"ws:01JYEJ5FX77TRNFK7V45VPSP3Y\",\"parentReferences\":[{\"rid\":\"rid:ws:5d805fac-dfeb-48cd-aad2-c0055438f880:default\",\"uid\":\"ws:01JYEJ5FX77TRNFK7V45VPSP3Y\"}]},\"roleBindings\":[{\"role\":\"workspace.edit\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]}]},{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}]}{\"policyList\":[{\"meta\":{\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\",\"parentReferences\":[{\"rid\":\"rid:org:5d805fac-dfeb-48cd-aad2-c0055438f880\",\"uid\":\"5d805fac-dfeb-48cd-aad2-c0055438f880\"}]},\"roleBindings\":[{\"role\":\"organization.admin\",\"subjects\":[{\"name\":\"csp:org_owner\",\"kind\":\"GROUP\"},{\"name\":\"tmc:admin\",\"kind\":\"GROUP\"}]},{\"role\":\"organization.credential.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]},{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"renw@vmware.com\",\"kind\":\"USER\"}]}]}]}2025-06-26 10:12:09 INFO  [SKIP] no direct rolebinding for workspaces:test-w2 is required to imported\n",
      "2025-06-26 10:12:09 INFO  Import access policies completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "./061-base-access-policies-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283047b1-a7a0-41ea-a169-ca57ea61ceb0",
   "metadata": {},
   "source": [
    "* Import Access Policies on Clusters and Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ce98c1d8-ce88-4602-8003-98f3c20009a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Import Policy Access on Clusters and Namespaces to TMC SM ...\n",
      "************************************************************************\n",
      "2025-06-26 10:49:33 INFO  Importing rolebindings on clusters ...\n",
      "2025-06-26 10:49:33 INFO  Importing access policies on cluster attached/attached/wc-02 ...\n",
      "{\"policy\":{\"meta\":{\"uid\":\"c:01JYNMX7KSP621MB6TFXE7V9TK\",\"parentReferences\":[{\"rid\":\"rid:c:5d805fac-dfeb-48cd-aad2-c0055438f880:attached:attached:wc-02\",\"uid\":\"c:01JYNMX7KSP621MB6TFXE7V9TK\"}]},\"roleBindings\":[{\"role\":\"cluster.admin\",\"subjects\":[{\"name\":\"willis.ren@broadcom.com\",\"kind\":\"USER\"}]}]}}2025-06-26 10:49:34 INFO  [SKIP] no direct rolebinding for clusters:sc-803_testns_wc-01 is required to imported\n",
      "2025-06-26 10:49:34 INFO  [SKIP] no direct rolebinding for clusters:vks-02_testns_vks-02-cc-01 is required to imported\n",
      "2025-06-26 10:49:35 INFO  Importing rolebindings on namespaces ...\n",
      "{\"policy\":{\"meta\":{\"uid\":\"ns:01JYNP9NE2WQHCHS65YGS6C37B\",\"parentReferences\":[{\"rid\":\"rid:ns:5d805fac-dfeb-48cd-aad2-c0055438f880:attached:attached:wc-02:test-ns1\",\"uid\":\"ns:01JYNP9NE2WQHCHS65YGS6C37B\"}]},\"roleBindings\":[{\"role\":\"namespace.view\",\"subjects\":[{\"name\":\"tmc:member\",\"kind\":\"GROUP\"}]}]}}2025-06-26 10:49:36 INFO  [SKIP] no direct rolebinding for clusters/wc-02/namespaces:sc-803_testns_wc-01_testns is required to imported\n",
      "2025-06-26 10:49:36 INFO  Import access policies on clusters and namespaces completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "./061-cluster-access-policies-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e1777-32de-4b79-95f7-3e4a4f259838",
   "metadata": {},
   "source": [
    "#### ES-62: Import Policy Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "110b5a56-e5f5-4d5f-b8b9-ff9f1e813569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Import Policy Templates to TMC SM ...\n",
      "************************************************************************\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-block-rolebinding-subjects\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-https-ingress\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-block-resources\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-require-labels\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-external-ips\n",
      "2025-06-26 10:14:34 INFO  [SKIP] system template:tmc-block-nodeport-service\n",
      "2025-06-26 10:14:34 INFO  Importing policy template custom-external-ips ...\n",
      "[i] read object --> \n",
      "---\n",
      "apiVersion: templates.gatekeeper.sh/v1beta1\n",
      "kind: ConstraintTemplate\n",
      "metadata:\n",
      "  name: custom-external-ips\n",
      "spec:\n",
      "  crd:\n",
      "    spec:\n",
      "      names:\n",
      "        kind: custom-external-ips\n",
      "      validation:\n",
      "        # Schema for the `parameters` field\n",
      "        openAPIV3Schema:\n",
      "          properties:\n",
      "            allowedIPs:\n",
      "              type: array\n",
      "              items:\n",
      "                type: string\n",
      "  targets:\n",
      "    - target: admission.k8s.gatekeeper.sh\n",
      "      rego: |\n",
      "        package tmcexternalips\n",
      "        violation[{\"msg\": msg}] {\n",
      "          input.review.kind.kind == \"Service\"\n",
      "          input.review.kind.group == \"\"\n",
      "          allowedIPs := {ip | ip := input.parameters.allowedIPs[_]}\n",
      "          externalIPs := {ip | ip := input.review.object.spec.externalIPs[_]}\n",
      "          forbiddenIPs := externalIPs - allowedIPs\n",
      "          # verify that there are no forbidden IPs set as Service spec.externalIPs\n",
      "          count(forbiddenIPs) > 0\n",
      "          msg := sprintf(\"service has forbidden external IPs: %v\", [forbiddenIPs])\n",
      "        }\n",
      "        violation[{\"msg\": msg}] {\n",
      "          input.review.kind.kind == \"Service\"\n",
      "          input.review.kind.group == \"\"\n",
      "          allowedIPs := {ip | ip := input.parameters.allowedIPs[_]}\n",
      "          loadBalancerIP := input.review.object.spec.loadBalancerIP\n",
      "          # verify that Service spec.loadBalancerIP does not have a forbidden IP\n",
      "          not allowedIPs[loadBalancerIP]\n",
      "          msg := sprintf(\"service has forbidden loadBalancer IP: %v\", [loadBalancerIP])\n",
      "        }\n",
      "\n",
      "---\n",
      "\u001b[31mx\u001b[0m rpc error: code = AlreadyExists desc = failed to create policy template \n",
      "2025-06-26 10:14:36 \u001b[0;31mERROR\u001b[0m Import policy templates exited with an error. \u001b[0;31m✖\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "./062-base-policy-templates-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f2a77-e63b-45a4-9c3a-b43e4b9b3938",
   "metadata": {},
   "source": [
    "#### ES-63: Import Policy Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354d18c-a996-4cce-a6ff-68ad9b61248b",
   "metadata": {},
   "source": [
    "* Import policy assignments on organizations, clustergroups and workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bac0282e-d6bd-43fc-8f0e-2e112df71c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Import Policy Assignments to TMC SM ...\n",
      "************************************************************************\n",
      "2025-06-26 10:14:40 INFO  Importing policies on organization ...\n",
      "\u001b[31m✖\u001b[0m  rpc error: code = AlreadyExists desc = failed to create policy \n",
      "2025-06-26 10:14:40 INFO  Importing policies on clustergroups ...\n",
      "utils/common.sh: line 9: pushd: /home/kubo/tmc-saas-migration-scripts/data/policies/assignments/../data/policies/assignments/clustergroups: No such file or directory\n",
      "ls: cannot access '*.yaml': No such file or directory\n",
      "utils/common.sh: line 13: popd: directory stack empty\n",
      "ls: cannot access '*.yaml': No such file or directory\n",
      "2025-06-26 10:14:40 INFO  Importing policies on workspaces ...\n",
      "utils/common.sh: line 9: pushd: /home/kubo/tmc-saas-migration-scripts/data/policies/assignments/../data/policies/assignments/workspaces: No such file or directory\n",
      "ls: cannot access '*.yaml': No such file or directory\n",
      "utils/common.sh: line 13: popd: directory stack empty\n",
      "ls: cannot access '*.yaml': No such file or directory\n",
      "2025-06-26 10:14:40 INFO  Import policy assignments on clusters completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "./063-base-policy-assignments-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d03b2f-d606-4a0d-ad86-2f437e60e2dd",
   "metadata": {},
   "source": [
    "* Import policy assignments on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eb002db1-b255-4f78-9c64-bfc3758ee103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "* Import Policy Assignments on Clusters to TMC SM ...\n",
      "************************************************************************\n",
      "2025-06-26 10:38:33 INFO  [SKIP] inherited policy:security-01\n",
      "2025-06-26 10:38:33 INFO  [SKIP] inherited policy:security-01\n",
      "2025-06-26 10:38:33 INFO  Importing policy assignment on cluster attached/attached/wc-02 ...\n",
      "\u001b[31m✖\u001b[0m  rpc error: code = AlreadyExists desc = failed to create policy \n",
      "2025-06-26 10:38:35 INFO  Import policy assignments on clusters completed successfully! \u001b[0;32m✔\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "./063-cluster-policy-assignments-import.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875255e8-f496-4eb7-8bf4-5dd82c6476b0",
   "metadata": {},
   "source": [
    "### Import data protection resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabe8e7-f82b-45ba-8aa8-6bb67ee6d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "Start loading backup_location_org.yaml, total number is 3\n",
      "************************************************************************\n",
      "==> Loading 0/3 in backup_location_org.yaml ......\n",
      "fullName:\n",
      "  name: dp-backup-location\n",
      "  providerName: tmc\n",
      "meta:\n",
      "  annotations: {}\n",
      "  creationTime: \"2025-06-26T03:21:20.826599Z\"\n",
      "  description: \"\"\n",
      "  generation: \"0\"\n",
      "  labels:\n",
      "    tmc.cloud.vmware.com/creator: hxie\n",
      "    tmc.cloud.vmware.com/customer-managed: \"true\"\n",
      "    tmc.cloud.vmware.com/managed: \"true\"\n",
      "  parentReferences: []\n",
      "  resourceVersion: \"1\"\n",
      "  uid: 01JYN424NR9B7N1FH42Y3TP8YK\n",
      "  updateTime: \"2025-06-26T03:21:20.837889Z\"\n",
      "spec:\n",
      "  assignedGroups:\n",
      "    - cluster:\n",
      "        managementClusterName: sc-803\n",
      "        name: wc-01\n",
      "        provisionerName: testns\n",
      "  bucket: bk1\n",
      "  caCert: \"\"\n",
      "  config:\n",
      "    s3Config:\n",
      "      publicUrl: http://10.159.151.243:9000\n",
      "      s3ForcePathStyle: true\n",
      "      s3Url: http://10.159.151.243:9000\n",
      "  credential:\n",
      "    name: hxie-minio-acct\n",
      "  region: bj\n",
      "  targetProvider: AWS\n",
      "type:\n",
      "  kind: BackupLocation\n",
      "  package: vmware.tanzu.manage.v1alpha1.dataprotection.provider.backuplocation\n",
      "  version: v1alpha1\n",
      " ================== >>>>>>>>>>> \u001b[31mx\u001b[0m rpc error: code = AlreadyExists desc = target location already defined with name - 'dp-backup-location' under the organization - '5d805fac-dfeb-48cd-aad2-c0055438f880' \n",
      "==> Loading 1/3 in backup_location_org.yaml ......\n",
      "fullName:\n",
      "  name: dp-migration-s3-location\n",
      "  providerName: tmc\n",
      "meta:\n",
      "  annotations: {}\n",
      "  creationTime: \"2025-06-18T10:22:54.507786Z\"\n",
      "  description: \"\"\n",
      "  generation: \"0\"\n",
      "  labels:\n",
      "    tmc.cloud.vmware.com/managed: \"true\"\n",
      "  parentReferences: []\n",
      "  resourceVersion: \"1\"\n",
      "  uid: 01JY1909HAF59Z4R96T7NM0S0R\n",
      "  updateTime: \"2025-06-25T05:53:12.042565Z\"\n",
      "spec:\n",
      "  assignedGroups:\n",
      "    - clustergroup:\n",
      "        name: default\n",
      "  bucket: tmc-dp-migration\n",
      "  caCert: \"\"\n",
      "  config:\n",
      "    s3Config:\n",
      "      publicUrl: https://tmc-dp-migration.s3.us-east-2.amazonaws.com\n",
      "      s3ForcePathStyle: true\n",
      "      s3Url: https://tmc-dp-migration.s3.us-east-2.amazonaws.com\n",
      "  credential:\n",
      "    name: dp-for-migration\n",
      "  region: us-east-2\n",
      "  targetProvider: AWS\n",
      "type:\n",
      "  kind: BackupLocation\n",
      "  package: vmware.tanzu.manage.v1alpha1.dataprotection.provider.backuplocation\n",
      "  version: v1alpha1\n",
      " ================== >>>>>>>>>>> \u001b[31mx\u001b[0m rpc error: code = AlreadyExists desc = target location already defined with name - 'dp-migration-s3-location' under the organization - '5d805fac-dfeb-48cd-aad2-c0055438f880' \n",
      "==> Loading 2/3 in backup_location_org.yaml ......\n",
      "fullName:\n",
      "  name: my-location-01\n",
      "  providerName: tmc\n",
      "meta:\n",
      "  annotations: {}\n",
      "  creationTime: \"2025-06-19T08:14:23.215347Z\"\n",
      "  description: \"\"\n",
      "  generation: \"0\"\n",
      "  labels:\n",
      "    tmc.cloud.vmware.com/creator: lichao.xue_broadcom.com\n",
      "    tmc.cloud.vmware.com/customer-managed: \"true\"\n",
      "    tmc.cloud.vmware.com/managed: \"true\"\n",
      "  parentReferences: []\n",
      "  resourceVersion: \"1\"\n",
      "  uid: 01JY3M1NZEYY041NACGN8GEEBH\n",
      "  updateTime: \"2025-06-19T08:14:23.223887Z\"\n",
      "spec:\n",
      "  assignedGroups:\n",
      "    - clustergroup:\n",
      "        name: test-cg1\n",
      "  bucket: velero\n",
      "  caCert: \"\"\n",
      "  config:\n",
      "    s3Config:\n",
      "      publicUrl: http://10.160.251.219:9000\n",
      "      s3ForcePathStyle: true\n",
      "      s3Url: http://10.160.251.219:9000\n",
      "  credential:\n",
      "    name: test\n",
      "  region: minio\n",
      "  targetProvider: AWS\n",
      "type:\n",
      "  kind: BackupLocation\n",
      "  package: vmware.tanzu.manage.v1alpha1.dataprotection.provider.backuplocation\n",
      "  version: v1alpha1\n",
      " ================== >>>>>>>>>>> \u001b[31mx\u001b[0m rpc error: code = AlreadyExists desc = target location already defined with name - 'my-location-01' under the organization - '5d805fac-dfeb-48cd-aad2-c0055438f880' \n",
      "==> No file dataprotection_clustergroups.yaml\n",
      "==> No file dataprotection_clusters.yaml\n",
      "==> No file schedule-clustergroup.yaml\n",
      "==> No data in schedule-cluster.yaml\n",
      "************************************************************************\n",
      "Start loading backup.yaml, total number is 1\n",
      "************************************************************************\n",
      "==> Loading 0/1 in backup.yaml ......\n",
      "fullName:\n",
      "  clusterName: wc-01\n",
      "  managementClusterName: sc-803\n",
      "  name: wc-01-backup-01\n",
      "  provisionerName: testns\n",
      "meta:\n",
      "  annotations: {}\n",
      "  creationTime: \"2025-06-26T03:51:13.772055Z\"\n",
      "  description: \"\"\n",
      "  generation: \"0\"\n",
      "  labels:\n",
      "    tmc.cloud.vmware.com/creator: hxie\n",
      "    tmc.cloud.vmware.com/managed: \"true\"\n",
      "  parentReferences: []\n",
      "  resourceVersion: \"1\"\n",
      "  uid: 01JYN5RVKB72N703BFA24JWKE7\n",
      "  updateTime: \"2025-06-26T06:17:04.167933Z\"\n",
      "spec:\n",
      "  csiSnapshotTimeout: 600s\n",
      "  defaultVolumesToFsBackup: true\n",
      "  defaultVolumesToRestic: null\n",
      "  excludedClusterScopedResources: []\n",
      "  excludedNamespaceScopedResources: []\n",
      "  excludedNamespaces:\n",
      "    - kube-system\n",
      "    - tanzu-system\n",
      "    - tkg-system\n",
      "    - velero\n",
      "    - vmware-system-csi\n",
      "    - vmware-system-tmc\n",
      "  excludedResources: []\n",
      "  hooks: null\n",
      "  includeClusterResources: null\n",
      "  includedClusterScopedResources:\n",
      "    - '*'\n",
      "  includedNamespaceScopedResources: []\n",
      "  includedNamespaces: []\n",
      "  includedResources: []\n",
      "  labelSelector: null\n",
      "  orLabelSelectors: []\n",
      "  orderedResources: {}\n",
      "  snapshotMoveData: false\n",
      "  snapshotVolumes: false\n",
      "  storageLocation: dp-backup-location\n",
      "  ttl: 2592000s\n",
      "  volumeSnapshotLocations: []\n",
      "type:\n",
      "  kind: Backup\n",
      "  package: vmware.tanzu.manage.v1alpha1.cluster.dataprotection.backup\n",
      "  version: v1alpha1\n",
      "\u001b[31mx\u001b[0m rpc error: code = FailedPrecondition desc = The backup location - 'dp-backup-location' under Org: '5d805fac-dfeb-48cd-aad2-c0055438f880' and Cluster: 'c:01JYNN2RYYNWC1CMJDSRGC6RQE' is not yet available for backup\n",
      "try again 0/60 ...\n",
      "\u001b[31mx\u001b[0m rpc error: code = FailedPrecondition desc = The backup location - 'dp-backup-location' under Org: '5d805fac-dfeb-48cd-aad2-c0055438f880' and Cluster: 'c:01JYNN2RYYNWC1CMJDSRGC6RQE' is not yet available for backup\n",
      "try again 1/60 ...\n",
      "\u001b[31mx\u001b[0m rpc error: code = FailedPrecondition desc = The backup location - 'dp-backup-location' under Org: '5d805fac-dfeb-48cd-aad2-c0055438f880' and Cluster: 'c:01JYNN2RYYNWC1CMJDSRGC6RQE' is not yet available for backup\n",
      "try again 2/60 ...\n",
      "\u001b[31mx\u001b[0m rpc error: code = FailedPrecondition desc = The backup location - 'dp-backup-location' under Org: '5d805fac-dfeb-48cd-aad2-c0055438f880' and Cluster: 'c:01JYNN2RYYNWC1CMJDSRGC6RQE' is not yet available for backup\n",
      "try again 3/60 ...\n",
      "\u001b[31mx\u001b[0m rpc error: code = FailedPrecondition desc = The backup location - 'dp-backup-location' under Org: '5d805fac-dfeb-48cd-aad2-c0055438f880' and Cluster: 'c:01JYNN2RYYNWC1CMJDSRGC6RQE' is not yet available for backup\n",
      "try again 4/60 ...\n"
     ]
    }
   ],
   "source": [
    "./064-cluster-data_protection-import.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de033fe6-d757-4a02-a639-115353d612f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
